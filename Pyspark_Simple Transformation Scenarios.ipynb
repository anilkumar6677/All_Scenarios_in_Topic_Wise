{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d42e6fb-88fb-4482-b1a3-006e5d547b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "âš¡ Simple Transformation Scenarios\n",
    "\n",
    "ðŸ”¹ Scenario 1 â€” Select Specific Columns\n",
    "\n",
    "Input DataFrame\n",
    "```\n",
    "+--------+--------+--------+--------+\n",
    "| emp_id | name   | dept   | salary |\n",
    "+--------+--------+--------+--------+\n",
    "| 1      | Amit   | IT     | 70000  |\n",
    "| 2      | Riya   | HR     | 90000  |\n",
    "| 3      | Karan  | IT     | 65000  |\n",
    "+--------+--------+--------+--------+\n",
    "```\n",
    "Transformation\n",
    "\n",
    "Select only name and salary.\n",
    "\n",
    "Output\n",
    "```\n",
    "+--------+--------+\n",
    "| name   | salary |\n",
    "+--------+--------+\n",
    "| Amit   | 70000  |\n",
    "| Riya   | 90000  |\n",
    "| Karan  | 65000  |\n",
    "+--------+--------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "defe470c-9708-4654-9666-717fcadd50fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  #''' import spark session from pyspark.sql(\n",
    "from pyspark.sql.functions import *  ## import functions as F from pyspark.sql.functions\n",
    "\n",
    "data = [\n",
    "    (1, \"Amit\", \"IT\", 70000),\n",
    "    (2, \"Riya\", \"HR\", 90000),\n",
    "    (3, \"Karan\", \"IT\", 65000)\n",
    "]\n",
    "\n",
    "# Column names\n",
    "columns = [\"emp_id\", \"name\", \"dept\", \"salary\"]\n",
    "\n",
    "\n",
    "spark= SparkSession.builder.appName(\"My App\").getOrCreate()\n",
    "#'''in databricks this spark session already created default so no neeed to create. so you can directly using \"spark\" keyword )'''\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "## Type1 by using select\n",
    "\n",
    "selectdf = df.select(\"name\",\"salary\")\n",
    "print()\n",
    "print(\"using select caluse:\")\n",
    "selectdf.show()\n",
    "\n",
    "## Type 2 by using sql format\n",
    "\n",
    "## if we want to use sql style data processing we need to create temparary view\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "sqldf= spark.sql(\"select name, salary from employees\")\n",
    "print(\"using sql way:\")\n",
    "sqldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56f6daad-b061-4c18-af78-74c135911714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8017cfe-e0c5-4d9f-a3dc-5235fe2c6ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "ðŸ”¹ Scenario 2 â€” Filter Rows\n",
    "\n",
    "Input DataFrame\n",
    "```\n",
    "+--------+--------+--------+--------+\n",
    "| emp_id | name   | dept   | salary |\n",
    "+--------+--------+--------+--------+\n",
    "| 1      | Amit   | IT     | 70000  |\n",
    "| 2      | Riya   | HR     | 90000  |\n",
    "| 3      | Karan  | IT     | 65000  |\n",
    "+--------+--------+--------+--------+\n",
    "```\n",
    "Transformation\n",
    "\n",
    "Filter employees with salary > 70000.\n",
    "\n",
    "Output\n",
    "```\n",
    "+--------+--------+--------+--------+\n",
    "| emp_id | name   | dept   | salary |\n",
    "+--------+--------+--------+--------+\n",
    "| 2      | Riya   | HR     | 90000  |\n",
    "+--------+--------+--------+--------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1681464a-1218-4542-a4d7-b4caba210727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "data =[\n",
    "    (1,\"Amit\",\"IT\",70000),\n",
    "    (2,\"Riya\",\"HR\",90000),\n",
    "    (3,\"karan\",\"IT\",65000)\n",
    "]\n",
    "\n",
    "Columns= [\"emp_id\",\"name\",\"dept\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data,Columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "print(\"using filter command:\")\n",
    "fildf = df. filter(\"salary > 70000\")\n",
    "fildf.show()\n",
    "\n",
    "print(\"using col function in filter :\" )\n",
    "fill2 = df.filter(col(\"salary\") > 70000)\n",
    "fill2.show()\n",
    "\n",
    "\n",
    "\n",
    "employees = df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "print(\"usinf sql type filter:\")\n",
    "sqltypedf = spark.sql(\"select * from employees where salary > 70000\") \n",
    "sqltypedf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bec84087-b3da-40ea-b27e-64f11a10736f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "ðŸ”¹ Scenario 3 â€” Add New Column\n",
    "\n",
    "Input DataFrame\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | salary |\n",
    "+--------+--------+--------+\n",
    "| 1      | Amit   | 70000  |\n",
    "| 2      | Riya   | 90000  |\n",
    "| 3      | Karan  | 65000  |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "Transformation\n",
    "\n",
    "Add a new column bonus = salary * 0.1.\n",
    "\n",
    "Output\n",
    "```\n",
    "+--------+--------+--------+-------+\n",
    "| emp_id | name   | salary | bonus |\n",
    "+--------+--------+--------+-------+\n",
    "| 1      | Amit   | 70000  | 7000  |\n",
    "| 2      | Riya   | 90000  | 9000  |\n",
    "| 3      | Karan  | 65000  | 6500  |\n",
    "+--------+--------+--------+-------+\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b062c7b-1f4f-406a-8862-255ec6b1573d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "data =[\n",
    "    (1,\"Amit\",\"IT\",70000),\n",
    "    (2,\"Riya\",\"HR\",90000),\n",
    "    (3,\"karan\",\"IT\",65000)\n",
    "]\n",
    "\n",
    "Columns= [\"emp_id\",\"name\",\"dept\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data,Columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "print(\"using with column and col function:\")\n",
    "adddf = df.withColumn(\"bouns\", (col(\"salary\") * 0.1).cast(\"int\"))\n",
    "adddf.show()\n",
    "\n",
    "employees = df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "print(\"using sql type with cast function\")\n",
    "sqlstyledf = spark.sql(\"\"\"select *, cast(salary * 0.1 as int) as Bouns from employees\"\"\")\n",
    "sqlstyledf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c7b2efa-5eee-4672-915f-b6dde9625cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¹ Scenario 4 â€” Group By and Aggregate\n",
    "\n",
    "Input DataFrame\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | dept   | salary |\n",
    "+--------+--------+--------+\n",
    "| 1      | IT     | 70000  |\n",
    "| 2      | HR     | 90000  |\n",
    "| 3      | IT     | 65000  |\n",
    "| 4      | HR     | 85000  |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "Transformation\n",
    "\n",
    "Group by dept and compute total salary.\n",
    "\n",
    "Output\n",
    "```\n",
    "+--------+------------+\n",
    "| dept   | total_salary|\n",
    "+--------+------------+\n",
    "| IT     | 135000     |\n",
    "| HR     | 175000     |\n",
    "+--------+------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8cc1c12-7265-4c92-aaee-c38e7c6d051c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¹ Scenario 5 â€” Rename Column\n",
    "\n",
    "Input DataFrame\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | salary |\n",
    "+--------+--------+--------+\n",
    "| 1      | Amit   | 70000  |\n",
    "| 2      | Riya   | 90000  |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "Transformation\n",
    "\n",
    "Rename salary â†’ monthly_salary.\n",
    "\n",
    "Output\n",
    "```\n",
    "+--------+--------+---------------+\n",
    "| emp_id | name   | monthly_salary|\n",
    "+--------+--------+---------------+\n",
    "| 1      | Amit   | 70000         |\n",
    "| 2      | Riya   | 90000         |\n",
    "+--------+--------+---------------+\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark_Simple Transformation Scenarios",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
