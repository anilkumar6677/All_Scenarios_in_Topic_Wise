{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d42e6fb-88fb-4482-b1a3-006e5d547b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "âš¡ Simple Transformation Scenarios\n",
    "\n",
    "ðŸ”¹ Scenario 1 â€” Select Specific Columns\n",
    "\n",
    "Input DataFrame\n",
    "```\n",
    "+--------+--------+--------+--------+\n",
    "| emp_id | name   | dept   | salary |\n",
    "+--------+--------+--------+--------+\n",
    "| 1      | Amit   | IT     | 70000  |\n",
    "| 2      | Riya   | HR     | 90000  |\n",
    "| 3      | Karan  | IT     | 65000  |\n",
    "+--------+--------+--------+--------+\n",
    "```\n",
    "Transformation\n",
    "\n",
    "Select only name and salary.\n",
    "\n",
    "Output\n",
    "```\n",
    "+--------+--------+\n",
    "| name   | salary |\n",
    "+--------+--------+\n",
    "| Amit   | 70000  |\n",
    "| Riya   | 90000  |\n",
    "| Karan  | 65000  |\n",
    "+--------+--------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "defe470c-9708-4654-9666-717fcadd50fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  #''' import spark session from pyspark.sql(\n",
    "from pyspark.sql.functions import *  ## import functions as F from pyspark.sql.functions\n",
    "\n",
    "data = [\n",
    "    (1, \"Amit\", \"IT\", 70000),\n",
    "    (2, \"Riya\", \"HR\", 90000),\n",
    "    (3, \"Karan\", \"IT\", 65000)\n",
    "]\n",
    "\n",
    "# Column names\n",
    "columns = [\"emp_id\", \"name\", \"dept\", \"salary\"]\n",
    "\n",
    "\n",
    "spark= SparkSession.builder.appName(\"My App\").getOrCreate()\n",
    "#'''in databricks this spark session already created default so no neeed to create. so you can directly using \"spark\" keyword )'''\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "# df.createOrReplaceTempView(\"employees\")\n",
    "## Type1 by using select\n",
    "\n",
    "selectdf = df.select(\"name\",\"salary\")\n",
    "# sqldf = spark.sql(\"select name,emp_id,dept from employees\")\n",
    "# sqldf.show()\n",
    "print()\n",
    "print(\"using select caluse:\")\n",
    "selectdf.show()\n",
    "\n",
    "## Type 2 by using sql format\n",
    "\n",
    "## if we want to use sql style data processing we need to create temparary view\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "sqldf= spark.sql(\"select name, salary from employees\")\n",
    "print(\"using sql way:\")\n",
    "sqldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56f6daad-b061-4c18-af78-74c135911714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8017cfe-e0c5-4d9f-a3dc-5235fe2c6ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "ðŸ”¹ Scenario 2 â€” Filter Rows\n",
    "\n",
    "Input DataFrame\n",
    "```\n",
    "+--------+--------+--------+--------+\n",
    "| emp_id | name   | dept   | salary |\n",
    "+--------+--------+--------+--------+\n",
    "| 1      | Amit   | IT     | 70000  |\n",
    "| 2      | Riya   | HR     | 90000  |\n",
    "| 3      | Karan  | IT     | 65000  |\n",
    "+--------+--------+--------+--------+\n",
    "```\n",
    "Transformation\n",
    "\n",
    "Filter employees with salary > 70000.\n",
    "\n",
    "Output\n",
    "```\n",
    "+--------+--------+--------+--------+\n",
    "| emp_id | name   | dept   | salary |\n",
    "+--------+--------+--------+--------+\n",
    "| 2      | Riya   | HR     | 90000  |\n",
    "+--------+--------+--------+--------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1681464a-1218-4542-a4d7-b4caba210727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import filter, col\n",
    "\n",
    "\n",
    "data =[\n",
    "    (1,\"Amit\",\"IT\",70000),\n",
    "    (2,\"Riya\",\"HR\",90000),\n",
    "    (3,\"karan\",\"IT\",65000)\n",
    "]\n",
    "\n",
    "Columns= [\"emp_id\",\"name\",\"dept\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data,Columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "print(\"using filter command:\")\n",
    "fildf = df.filter(\"salary > 70000\")\n",
    "fildf.show()\n",
    "\n",
    "print(\"using col function in filter :\" )\n",
    "fill2 = df.filter(col(\"salary\") > 70000)\n",
    "fill2.show()\n",
    "\n",
    "\n",
    "\n",
    "employees = df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "print(\"usinf sql type filter:\")\n",
    "sqltypedf = spark.sql(\"select * from employees where salary > 70000\") \n",
    "sqltypedf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bec84087-b3da-40ea-b27e-64f11a10736f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "ðŸ”¹ Scenario 3 â€” Add New Column\n",
    "\n",
    "Input DataFrame\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | salary |\n",
    "+--------+--------+--------+\n",
    "| 1      | Amit   | 70000  |\n",
    "| 2      | Riya   | 90000  |\n",
    "| 3      | Karan  | 65000  |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "Transformation\n",
    "\n",
    "Add a new column bonus = salary * 0.1.\n",
    "\n",
    "Output\n",
    "```\n",
    "+--------+--------+--------+-------+\n",
    "| emp_id | name   | salary | bonus |\n",
    "+--------+--------+--------+-------+\n",
    "| 1      | Amit   | 70000  | 7000  |\n",
    "| 2      | Riya   | 90000  | 9000  |\n",
    "| 3      | Karan  | 65000  | 6500  |\n",
    "+--------+--------+--------+-------+\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b062c7b-1f4f-406a-8862-255ec6b1573d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "data =[\n",
    "    (1,\"Amit\",\"IT\",70000),\n",
    "    (2,\"Riya\",\"HR\",90000),\n",
    "    (3,\"karan\",\"IT\",65000)\n",
    "]\n",
    "\n",
    "Columns= [\"emp_id\",\"name\",\"dept\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data,Columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "print(\"using withcolumn and col function:\")\n",
    "adddf = df.withColumn(\"bouns\", (col(\"salary\") * (10/100)).cast(\"int\"))\n",
    "adddf.show()\n",
    "\n",
    "employees = df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "print(\"using sql type with cast function\")\n",
    "sqlstyledf = spark.sql(\"\"\"select *, cast(salary * 0.1 as int) as Bouns from employees\"\"\")\n",
    "sqlstyledf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c7b2efa-5eee-4672-915f-b6dde9625cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¹ Scenario 4 â€” Group By and Aggregate\n",
    "\n",
    "Input DataFrame\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | dept   | salary |\n",
    "+--------+--------+--------+\n",
    "| 1      | IT     | 70000  |\n",
    "| 2      | HR     | 90000  |\n",
    "| 3      | IT     | 65000  |\n",
    "| 4      | HR     | 85000  |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "Transformation\n",
    "\n",
    "Group by dept and compute total salary.\n",
    "\n",
    "Output\n",
    "```\n",
    "+--------+------------+\n",
    "| dept   | total_salary|\n",
    "+--------+------------+\n",
    "| IT     | 135000     |\n",
    "| HR     | 175000     |\n",
    "+--------+------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00cbfb98-75f0-4d35-b087-443b294ec4e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, filter, sum\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeData\").getOrCreate()\n",
    "\n",
    "# Input data\n",
    "data = [\n",
    "    (1, \"IT\", 70000),\n",
    "    (2, \"HR\", 90000),\n",
    "    (3, \"IT\", 65000),\n",
    "    (4, \"HR\", 85000)\n",
    "]\n",
    "\n",
    "# Create DataFrame without StructType\n",
    "df = spark.createDataFrame(data, [\"emp_id\", \"dept\", \"salary\"])\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "\n",
    "groupdf = df.groupBy(\"dept\",\"emp_id\").agg(sum(col(\"salary\")).alias(\"total_salary\"))\n",
    "groupdf.show()\n",
    "\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "sqlgroupdf = spark.sql(\"\"\"\n",
    "                       select  dept, sum(salary) as Total_salary from df\n",
    "                       group by dept\n",
    "                       \"\"\")\n",
    "sqlgroupdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8cc1c12-7265-4c92-aaee-c38e7c6d051c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¹ Scenario 5 â€” Rename Column\n",
    "\n",
    "Input DataFrame\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | salary |\n",
    "+--------+--------+--------+\n",
    "| 1      | Amit   | 70000  |\n",
    "| 2      | Riya   | 90000  |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "Transformation\n",
    "\n",
    "Rename salary â†’ monthly_salary.\n",
    "\n",
    "Output\n",
    "```\n",
    "+--------+--------+---------------+\n",
    "| emp_id | name   | monthly_salary|\n",
    "+--------+--------+---------------+\n",
    "| 1      | Amit   | 70000         |\n",
    "| 2      | Riya   | 90000         |\n",
    "+--------+--------+---------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a85166-473e-4fe4-b093-3295b0246dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeNameData\").getOrCreate()\n",
    "\n",
    "# Input data\n",
    "data = [\n",
    "    (1, \"Amit\", 70000),\n",
    "    (2, \"Riya\", 90000)\n",
    "]\n",
    "\n",
    "# Create DataFrame without StructType\n",
    "df = spark.createDataFrame(data, [\"emp_id\", \"name\", \"salary\"])\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "\n",
    "renamedf = df.withColumnRenamed(\"salary\",\"monthly_salary\").show()\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "sqlrenamedf = spark.sql(\"\"\"\n",
    "                        select emp_id, name, salary as monthly_salary from df\n",
    "                        \"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e5ff84a-0d52-4b5e-99d4-4dcc11d81e80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "ðŸ”¥ Scenario 1 â€” Inner Join: Employees With Matching Departments\n",
    "\n",
    "ðŸ“˜ Description\n",
    "\n",
    "Perform an inner join to return only employees who have a matching department in the department table.\n",
    "\n",
    "ðŸ”¹ Input: employees\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | dept_id|\n",
    "+--------+--------+--------+\n",
    "| 1      | Amit   | 10     |\n",
    "| 2      | Riya   | 20     |\n",
    "| 3      | Karan  | 30     |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "ðŸ”¹ Input: departments\n",
    "```\n",
    "+--------+-------------+\n",
    "| dept_id| dept_name   |\n",
    "+--------+-------------+\n",
    "| 10     | IT          |\n",
    "| 20     | HR          |\n",
    "+--------+-------------+\n",
    "```\n",
    "ðŸ”¹ Output\n",
    "```\n",
    "+--------+--------+--------+-------------+\n",
    "| emp_id | name   | dept_id| dept_name   |\n",
    "+--------+--------+--------+-------------+\n",
    "| 1      | Amit   | 10     | IT          |\n",
    "| 2      | Riya   | 20     | HR          |\n",
    "+--------+--------+--------+-------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da49dada-cf7f-4823-9d92-4e32c00f6647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeesData\").getOrCreate()\n",
    "\n",
    "# Input data\n",
    "employees_data = [\n",
    "    (1, \"Amit\", 10),\n",
    "    (2, \"Riya\", 20),\n",
    "    (3, \"Karan\", 30)\n",
    "]\n",
    "\n",
    "# Create DataFrame without StructType\n",
    "employees_df = spark.createDataFrame(\n",
    "    employees_data,\n",
    "    [\"emp_id\", \"name\", \"dept_id\"]\n",
    ")\n",
    "\n",
    "# Show DataFrame\n",
    "employees_df.show()\n",
    "\n",
    "\n",
    "departments_data = [\n",
    "    (10, \"IT\"),\n",
    "    (20, \"HR\")\n",
    "]\n",
    "\n",
    "# Create DataFrame without StructType\n",
    "departments_df = spark.createDataFrame(\n",
    "    departments_data,\n",
    "    [\"dept_id\", \"dept_name\"]\n",
    ")\n",
    "\n",
    "# Show DataFrame\n",
    "departments_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c5a15ba-3c12-46f4-a8a3-15c3692364cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¥ Scenario 2 â€” Left Join: Employees Without Departments\n",
    "\n",
    "ðŸ“˜ Description\n",
    "\n",
    "Find all employees and attach department names where available.\n",
    "Employees without a department should show NULL.\n",
    "\n",
    "ðŸ”¹ Input: employees\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | dept_id|\n",
    "+--------+--------+--------+\n",
    "| 1      | Amit   | 10     |\n",
    "| 2      | Riya   | 20     |\n",
    "| 3      | Karan  | 30     |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "ðŸ”¹ Input: departments\n",
    "```\n",
    "+--------+-------------+\n",
    "| dept_id| dept_name   |\n",
    "+--------+-------------+\n",
    "| 10     | IT          |\n",
    "| 20     | HR          |\n",
    "+--------+-------------+\n",
    "```\n",
    "\n",
    "ðŸ”¹ Output\n",
    "```\n",
    "+--------+--------+--------+-------------+\n",
    "| emp_id | name   | dept_id| dept_name   |\n",
    "+--------+--------+--------+-------------+\n",
    "| 1      | Amit   | 10     | IT          |\n",
    "| 2      | Riya   | 20     | HR          |\n",
    "| 3      | Karan  | 30     | NULL        |\n",
    "+--------+--------+--------+-------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c409d8-5870-41dc-a0e0-0f5ff30037e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeesData\").getOrCreate()\n",
    "\n",
    "# Input data\n",
    "employees_data = [\n",
    "    (1, \"Amit\", 10),\n",
    "    (2, \"Riya\", 20),\n",
    "    (3, \"Karan\", 30)\n",
    "]\n",
    "\n",
    "# Create DataFrame without StructType\n",
    "employees_df = spark.createDataFrame(\n",
    "    employees_data,\n",
    "    [\"emp_id\", \"name\", \"dept_id\"]\n",
    ")\n",
    "\n",
    "# Show DataFrame\n",
    "employees_df.show()\n",
    "\n",
    "\n",
    "departments_data = [\n",
    "    (10, \"IT\"),\n",
    "    (20, \"HR\")\n",
    "]\n",
    "\n",
    "# Create DataFrame without StructType\n",
    "departments_df = spark.createDataFrame(\n",
    "    departments_data,\n",
    "    [\"dept_id\", \"dept_name\"]\n",
    ")\n",
    "\n",
    "# Show DataFrame\n",
    "departments_df.show()\n",
    "\n",
    "\n",
    "joindf = employees_df.join(departments_df, [\"dept_id\"], \"left\")\n",
    "joindf.show()\n",
    "\n",
    "differentcoldf = employees_df.join(departments_df, (employees_df.dept_id1 == departments_df.dept_id2), \"left\")\\\n",
    "    .withColumnRenamed(\"dept_id1\", \"dept_id\").drop(\"dept_id2\")\n",
    "differentcoldf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebac5f4d-ae45-4929-88f5-6bebc93b463a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¥ Scenario 3 â€” Right Join: All Departments Even Without Employees\n",
    "\n",
    "ðŸ“˜ Description\n",
    "\n",
    "Return every department, even if no employee belongs to it.\n",
    "\n",
    "ðŸ”¹ Input: employees\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | dept_id|\n",
    "+--------+--------+--------+\n",
    "| 1      | Amit   | 10     |\n",
    "| 2      | Riya   | 20     |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "ðŸ”¹ Input: departments\n",
    "```\n",
    "+--------+-------------+\n",
    "| dept_id| dept_name   |\n",
    "+--------+-------------+\n",
    "| 10     | IT          |\n",
    "| 20     | HR          |\n",
    "| 30     | Finance     |\n",
    "+--------+-------------+\n",
    "```\n",
    "ðŸ”¹ Output\n",
    "```\n",
    "+--------+--------+--------+-------------+\n",
    "| emp_id | name   | dept_id| dept_name   |\n",
    "+--------+--------+--------+-------------+\n",
    "| 1      | Amit   | 10     | IT          |\n",
    "| 2      | Riya   | 20     | HR          |\n",
    "| NULL   | NULL   | 30     | Finance     |\n",
    "+--------+--------+--------+-------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41625379-5976-4cd0-ae31-506258a21583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeesData\").getOrCreate()\n",
    "\n",
    "# Input data\n",
    "employees_data = [\n",
    "    (1, \"Amit\", 10),\n",
    "    (2, \"Riya\", 20)\n",
    "]\n",
    "\n",
    "# Create DataFrame without StructType\n",
    "employees_df = spark.createDataFrame(\n",
    "    employees_data,\n",
    "    [\"emp_id\", \"name\", \"dept_id1\"]\n",
    ")\n",
    "\n",
    "# Show DataFrame\n",
    "employees_df.show()\n",
    "\n",
    "\n",
    "departments_data = [\n",
    "    (10, \"IT\"),\n",
    "    (20, \"HR\"),\n",
    "    (30, \"IT\")\n",
    "]\n",
    "\n",
    "# Create DataFrame without StructType\n",
    "departments_df = spark.createDataFrame(\n",
    "    departments_data,\n",
    "    [\"dept_id2\", \"dept_name\"]\n",
    ")\n",
    "\n",
    "# Show DataFrame\n",
    "departments_df.show()\n",
    "\n",
    "\n",
    "# joindf = employees_df.join(departments_df, [\"dept_id\"], \"right\")\n",
    "# joindf.show()\n",
    "\n",
    "differentcoldf = employees_df.join(departments_df, (employees_df.dept_id1 == departments_df.dept_id2), \"right\")\\\n",
    "     .withColumnRenamed(\"dept_id2\", \"dept_id\").drop(\"dept_id1\")\n",
    "   \n",
    "    \n",
    "differentcoldf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10886300-3cd7-4ade-ab9e-883b9fc9c01b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¥ Scenario 4 â€” Full Outer Join: All Employees & All Departments\n",
    "\n",
    "ðŸ“˜ Description\n",
    "\n",
    "Return every employee and every department.\n",
    "Missing matches should show NULL on either side.\n",
    "\n",
    "ðŸ”¹ Input: employees\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | dept_id|\n",
    "+--------+--------+--------+\n",
    "| 1      | Amit   | 10     |\n",
    "| 2      | Riya   | 20     |\n",
    "| 3      | Karan  | 30     |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "ðŸ”¹ Input: departments\n",
    "```\n",
    "+--------+-------------+\n",
    "| dept_id| dept_name   |\n",
    "+--------+-------------+\n",
    "| 10     | IT          |\n",
    "| 20     | HR          |\n",
    "| 40     | Support     |\n",
    "+--------+-------------+\n",
    "```\n",
    "ðŸ”¹ Output\n",
    "```\n",
    "+--------+--------+--------+-------------+\n",
    "| emp_id | name   | dept_id| dept_name   |\n",
    "+--------+--------+--------+-------------+\n",
    "| 1      | Amit   | 10     | IT          |\n",
    "| 2      | Riya   | 20     | HR          |\n",
    "| 3      | Karan  | 30     | NULL        |\n",
    "| NULL   | NULL   | 40     | Support     |\n",
    "+--------+--------+--------+-------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8adc042-877a-4cbf-9b0c-1204b57b30b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¥ Scenario 5 â€” Anti Join: Employees Without Matching Department\n",
    "\n",
    "ðŸ“˜ Description\n",
    "\n",
    "Find employees whose department is not present in the department table.\n",
    "\n",
    "ðŸ”¹ Input: employees\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | dept_id|\n",
    "+--------+--------+--------+\n",
    "| 1      | Amit   | 10     |\n",
    "| 2      | Riya   | 20     |\n",
    "| 3      | Karan  | 30     |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "ðŸ”¹ Input: departments\n",
    "```\n",
    "+--------+-------------+\n",
    "| dept_id| dept_name   |\n",
    "+--------+-------------+\n",
    "| 10     | IT          |\n",
    "| 20     | HR          |\n",
    "+--------+-------------+\n",
    "```\n",
    "ðŸ”¹ Output\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | dept_id|\n",
    "+--------+--------+--------+\n",
    "| 3      | Karan  | 30     |\n",
    "+--------+--------+--------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbfb41bc-3ee1-487c-8597-c1e30a174950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "ðŸ”¥ Scenario 6 â€” Semi Join: Employees With Existing Departments Only\n",
    "\n",
    "ðŸ“˜ Description\n",
    "\n",
    "A semi join returns rows from the left table (employees) that have matching rows in the right table (departments).\n",
    "Unlike inner join, it does not include columns from the right table.\n",
    "\n",
    "ðŸ”¹ Input: employees\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | dept_id|\n",
    "+--------+--------+--------+\n",
    "| 1      | Amit   | 10     |\n",
    "| 2      | Riya   | 20     |\n",
    "| 3      | Karan  | 30     |\n",
    "+--------+--------+--------+\n",
    "```\n",
    "ðŸ”¹ Input: departments\n",
    "```\n",
    "+--------+-------------+\n",
    "| dept_id| dept_name   |\n",
    "+--------+-------------+\n",
    "| 10     | IT          |\n",
    "| 20     | HR          |\n",
    "+--------+-------------+\n",
    "```\n",
    "ðŸ”¹ Output\n",
    "```\n",
    "+--------+--------+--------+\n",
    "| emp_id | name   | dept_id|\n",
    "+--------+--------+--------+\n",
    "| 1      | Amit   | 10     |\n",
    "| 2      | Riya   | 20     |\n",
    "+--------+--------+--------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d883235d-3ebd-403c-b588-0552fa681188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¥ Scenario 7 â€” Cross Join: Cartesian Product of Employees and Departments\n",
    "\n",
    "ðŸ“˜ Description\n",
    "\n",
    "A cross join returns all possible combinations of rows from both tables.\n",
    "Useful for generating all pairings.\n",
    "\n",
    "ðŸ”¹ Input: employees\n",
    "```\n",
    "+--------+--------+\n",
    "| emp_id | name   |\n",
    "+--------+--------+\n",
    "| 1      | Amit   |\n",
    "| 2      | Riya   |\n",
    "+--------+--------+\n",
    "```\n",
    "ðŸ”¹ Input: departments\n",
    "```\n",
    "+--------+-------------+\n",
    "| dept_id| dept_name   |\n",
    "+--------+-------------+\n",
    "| 10     | IT          |\n",
    "| 20     | HR          |\n",
    "+--------+-------------+\n",
    "```\n",
    "ðŸ”¹ Output\n",
    "```\n",
    "+--------+--------+--------+-------------+\n",
    "| emp_id | name   | dept_id| dept_name   |\n",
    "+--------+--------+--------+-------------+\n",
    "| 1      | Amit   | 10     | IT          |\n",
    "| 1      | Amit   | 20     | HR          |\n",
    "| 2      | Riya   | 10     | IT          |\n",
    "| 2      | Riya   | 20     | HR          |\n",
    "+--------+--------+--------+-------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28c086ff-6fd9-42f6-b738-821de77ec0fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¥ Scenario 8 â€” Self Join: Employees and Their Managers\n",
    "\n",
    "ðŸ“˜ Description\n",
    "\n",
    "A self join is when a table joins with itself.\n",
    "In this example, we assume employees have a manager_id pointing to another employee.\n",
    "\n",
    "ðŸ”¹ Input: employees\n",
    "```\n",
    "+--------+--------+-----------+\n",
    "| emp_id | name   | manager_id|\n",
    "+--------+--------+-----------+\n",
    "| 1      | Amit   | 3         |\n",
    "| 2      | Riya   | 3         |\n",
    "| 3      | Karan  | NULL      |\n",
    "+--------+--------+-----------+\n",
    "```\n",
    "ðŸ”¹ Output\n",
    "```\n",
    "+--------+--------+-----------+--------+\n",
    "| emp_id | name   | manager_id| manager|\n",
    "+--------+--------+-----------+--------+\n",
    "| 1      | Amit   | 3         | Karan  |\n",
    "| 2      | Riya   | 3         | Karan  |\n",
    "| 3      | Karan  | NULL      | NULL   |\n",
    "+--------+--------+-----------+--------+"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark_Simple Transformation Scenarios",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
